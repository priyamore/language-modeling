{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(f'{len(words)}')\n",
    "\n",
    "chars = list('abcdefghijklmnopqrstuvwxyz')\n",
    "stoi_lookup = {c: i+1 for i, c in enumerate(chars)}\n",
    "stoi_lookup['.'] = 0\n",
    "itos_lookups = {i: char for char, i in stoi_lookup.items()}\n",
    "\n",
    "# Dataset creation\n",
    "\n",
    "block_size = 3 # context length\n",
    "X, Y = [], []\n",
    "\n",
    "for word in words:\n",
    "    context = [0] * block_size\n",
    "    seq = word + '.'    # don't forget to add the . add the end\n",
    "\n",
    "    for char in seq:\n",
    "        X.append(context)\n",
    "        y_i = stoi_lookup[char]\n",
    "        Y.append(y_i)\n",
    "        context = context[1:] + [y_i]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "\n",
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Lookup table - initialized with random values in the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 3, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 27 chars emebedded into two dims randomly\n",
    "dims = 2\n",
    "C = torch.randn((27, dims))\n",
    "embs = C[X]\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dataset:  torch.Size([228146, 3]) torch.Size([228146])\n",
      "Number of parameters in the network:  3481\n"
     ]
    }
   ],
   "source": [
    "print(\"the dataset: \", X.shape, Y.shape)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "\n",
    "params = [C, W1, b1, W2, b2]\n",
    "\n",
    "print(\"Number of parameters in the network: \", sum(p.nelement() for p in params))\n",
    "\n",
    "# set the p.grad requires to true first so that we are able to get the grad for the params\n",
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(17.9727, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.8143, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.5994, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.9662, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.2549, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.7655, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.1703, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.7367, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.4205, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.7171, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.9041, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.1387, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.6838, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.6961, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.2888, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.7926, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.8958, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.6158, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.2679, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.6744, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.0332, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.7715, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.6952, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.6987, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.3910, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.6601, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.2075, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.8257, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.5513, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.9553, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.8859, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.6192, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.3703, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.2069, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.3824, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.3091, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.5387, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.9639, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.0967, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.3508, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.6831, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.1476, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.1890, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.4323, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.6085, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.4512, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.0859, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.7545, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.8921, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.0284, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.4285, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0573, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.9685, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.3567, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7712, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2968, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.3740, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7066, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.0048, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.8479, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7428, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.5922, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.1772, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2179, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.0181, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0232, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.1730, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5918, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.5412, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2338, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.7251, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.4820, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.8107, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.1762, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9636, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.3075, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9440, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2023, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.1394, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.0394, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7583, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.4263, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.2632, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9869, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2758, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.3278, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1582, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.4578, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.8095, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5580, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9315, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6666, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1156, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9656, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4827, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.6611, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0557, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5394, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.1604, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4066, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for step in range(100):\n",
    "    # forward pass\n",
    "    ix = torch.randint(0, X.shape[0], (32, ))\n",
    "    # make sure to only select the indices in the current mini batch\n",
    "    embs = C[X[ix]]\n",
    "    h = torch.tanh(embs.view(-1, block_size*dims) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    # only select the indices in the current mini batch\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    # backward pass\n",
    "    # set the gradients to 0 - IMP\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    \n",
    "    # calculate the gradients of the loss w.r.t params\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the params\n",
    "    for p in params:\n",
    "        l_rate = 0.1 # learning rate \n",
    "        p.data += -l_rate * p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we choose a good learning rate, \n",
    "- we can try different learning rates\n",
    "- plot the losses for those learning rates to understand which learning rate would be optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning rates\n",
    "lre = torch.linspace(-3, 0, 1000) \n",
    "lrs= 10 ** lre # 0.001 to 1\n",
    "lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets train the network with these learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(17.3253, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.5967, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(22.0757, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.9697, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.6514, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(20.0479, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.1620, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.9437, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(20.6769, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(22.5803, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.2446, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(21.8621, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(20.4696, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.4321, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.3917, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.3079, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.3496, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(20.5044, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.0476, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(22.2821, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.2118, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.8981, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(22.2488, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.8612, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.8491, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.1322, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.7350, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.9020, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.4207, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(21.1599, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(21.3033, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.3499, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.2670, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.7121, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.7113, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.7627, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.3901, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.6502, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.2768, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.3491, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.0632, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.7223, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.7036, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.9130, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.4769, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.2216, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(20.1570, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.0511, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.2084, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.7611, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.3233, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.8848, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.7000, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.1582, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.8022, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.7903, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.4202, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.1582, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.0577, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.9386, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.7374, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.6112, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.2016, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.9712, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.2819, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(20.2826, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.7782, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.5259, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.8202, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.8808, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.0898, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.7631, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.0301, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.8516, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.0365, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.9944, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.0287, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.7526, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.4764, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.2759, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.7969, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.9288, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.0616, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.4577, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.3510, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.4764, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.5380, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.7805, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.7004, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.7176, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.7513, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.1565, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.8004, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.4791, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.9448, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.1306, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.9640, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.5716, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.1376, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.0130, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.6841, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.1474, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.9752, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.9769, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.9874, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.5109, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.4969, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.3021, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.3217, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.6845, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.2765, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.7419, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.0652, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.5595, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.3008, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.9654, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.9562, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.7803, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.8126, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.5242, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.6312, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.4642, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.3236, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.6084, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.3921, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.5690, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.3919, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.3002, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.7282, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.2045, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(19.0105, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.2347, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.0270, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.4622, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.8902, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.3248, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.7334, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.5115, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.3334, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.4813, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.1993, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.9367, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.0234, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.7613, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.6228, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.5906, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.8370, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.8882, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.1317, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.9094, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.4584, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.2317, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.1067, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.7861, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.2387, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.7067, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.5832, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.4126, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.2504, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(18.6791, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.9775, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.6597, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.1841, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.3201, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.5227, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.7728, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.7453, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.0447, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.1241, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.2662, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.6749, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.8912, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.1259, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.8052, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.5867, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.0191, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.2933, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.0486, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.0015, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.0821, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.8367, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.0710, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(17.0385, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.7876, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.9324, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.7879, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(16.1725, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.2762, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.8540, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.9114, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.8951, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.1520, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.0977, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.2849, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.5347, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.0009, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.7350, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.3107, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.9580, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.3340, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.1057, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.8439, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.0715, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.9065, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.4260, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.4802, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.0968, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.7520, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.1814, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.0101, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.5417, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.4723, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.9143, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.5627, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.4276, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.2442, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.4331, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.7587, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.8275, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.5306, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.5517, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.7315, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.9483, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.2136, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.3720, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.9742, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.7571, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.3786, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.4638, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.4110, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.9623, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.7868, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.9570, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.8861, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.9020, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.3566, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.6874, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.9612, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.4459, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.3468, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.1682, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.8078, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.5049, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.6726, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.3534, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.7592, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.6699, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.2325, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.3907, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.7759, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(15.1754, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.6605, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.1935, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.3546, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.4304, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.3446, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.5147, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.8425, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.4938, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.8650, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.6963, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.1936, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.9167, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.4042, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.7519, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.0640, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(14.2165, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.5334, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.6984, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.7086, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.6118, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.1313, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.7244, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.8239, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.2484, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.7648, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.3793, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.6960, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.3855, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.3616, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(13.2022, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.3736, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.3585, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.1192, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.0663, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.3551, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.8168, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.7915, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.9342, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.2966, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.5831, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.9644, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.4082, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.2912, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.5681, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.7380, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.0605, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.7929, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.1061, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.8010, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.1269, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.2307, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.0108, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.1050, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.9238, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.7001, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.3413, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.2256, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.2717, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.6497, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.8730, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.7527, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.7837, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.7818, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.8149, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.8064, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.6728, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.2911, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.3648, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.5982, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.7876, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.9889, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.0242, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.7524, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.3434, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.7453, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.4825, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.4187, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.8082, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.1968, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.3669, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.1683, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.8654, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.3219, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.6407, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.0737, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.4934, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.9272, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.0521, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.3221, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.1201, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.0500, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.7374, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.8792, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.9887, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.0784, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.8405, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.7752, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.6053, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.4905, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.3965, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.2438, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(11.0507, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.0843, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.3943, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.5767, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.8103, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.0488, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.7423, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.6953, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.2124, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.2777, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.0535, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.0593, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.7064, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.7988, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.0443, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.6179, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.5511, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.3350, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.1461, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.4429, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.3440, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.9005, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.5416, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.5998, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.3784, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.2128, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.6640, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.4212, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.6653, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.1006, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.9849, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.5561, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.6354, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.4986, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.1474, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.9107, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(10.2157, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.5518, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.5077, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.4875, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.1717, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.9022, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.5201, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.8270, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.5455, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.9372, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.7055, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.9545, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.0503, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.6772, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.2596, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.7094, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.6530, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.0303, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.1124, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.9843, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.8002, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.8975, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.9091, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.7069, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.7013, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.3024, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.9639, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.1018, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.0239, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.3888, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.8680, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.0175, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.8117, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.7034, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.6853, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.1194, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.4311, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.0526, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.2055, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.6876, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.3363, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.2773, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.0648, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.3613, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.4560, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.9558, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.5934, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.3325, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.3486, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.8257, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.9119, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.5703, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.3654, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.8239, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.2893, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.3394, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.0047, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.5886, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.8449, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.2270, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.1766, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.0884, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.1694, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.7979, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.2984, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.2385, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.4435, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.9873, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.8462, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.1926, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.9812, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.1412, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.0160, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.6727, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.6076, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.4155, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.1263, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.8986, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.6438, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.1093, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.6633, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.6761, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.9036, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.6813, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7476, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.3471, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.9641, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.0463, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.6730, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.9836, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.8803, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.1685, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.7296, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.9336, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.8262, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.3060, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.2139, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.6523, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.2182, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.4199, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.4176, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.7216, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.0143, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.2892, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.3626, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.6855, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.3275, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.6072, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.9379, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.4879, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.8076, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.8222, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.5983, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.1340, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.2953, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.8766, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.9182, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.6544, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.0487, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.5877, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.7905, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.4352, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.7794, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.0444, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.8214, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7568, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.1561, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.2275, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.9953, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.8628, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.4865, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.0721, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.4717, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.8927, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.9323, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.9173, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.2735, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.0125, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.1062, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7415, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.2619, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.3844, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.6541, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.5961, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.8179, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.4395, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.5007, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2274, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.7289, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.8775, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2112, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.6062, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.6330, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0822, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.4099, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.6959, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.4135, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7626, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.3966, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.1485, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2236, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.8393, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7198, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.8742, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9904, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.6460, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.8814, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.0601, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.3218, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2812, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.4160, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7558, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9509, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.8563, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.3244, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.5312, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.4044, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.8592, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.5151, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9321, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.9703, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7118, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7828, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.1071, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5554, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.4280, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3841, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.4408, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6515, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6620, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3117, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5320, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.9767, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.3734, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.4920, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.7834, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6173, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.1075, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5456, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9476, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6724, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.1079, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7005, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.6616, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0158, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3268, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.8090, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7689, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6179, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9075, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6513, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7639, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3136, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.8250, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6750, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7186, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8618, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.1303, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7913, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7636, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2976, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6512, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.5343, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4527, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0891, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0519, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4425, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.0870, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0578, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5961, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6503, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4420, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6884, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0717, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8674, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3927, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0935, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9689, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6957, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5016, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5739, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6094, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5453, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2281, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4025, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4522, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9557, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5899, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8624, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4529, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0579, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7938, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7814, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1085, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1712, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9136, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0752, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5173, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8902, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2746, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4662, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7142, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.3397, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9748, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.5777, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1594, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0388, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8438, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7926, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7512, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7281, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5199, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1041, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.1420, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7369, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9891, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8640, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3743, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5978, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2621, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2446, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2044, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9899, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7570, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1260, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7481, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.3292, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0948, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5768, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1649, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9555, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7529, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4613, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2247, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4519, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4746, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2661, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1835, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7248, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2403, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7768, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.8516, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2665, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7024, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0453, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7552, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2575, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2258, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6901, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6925, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4269, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5995, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5658, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4541, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6469, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2936, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.3130, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.5820, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5890, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9673, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0006, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1475, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0177, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9924, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0622, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.3995, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9396, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4526, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7000, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1796, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6581, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2042, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3939, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7597, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1211, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4046, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5070, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1539, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1155, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7184, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1737, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0176, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3376, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8244, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4059, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0034, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.4641, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1859, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6295, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6871, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2450, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9782, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8772, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3147, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1419, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6272, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0306, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2326, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3767, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5053, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7676, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3021, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9979, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4302, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0616, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7593, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0377, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1281, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6575, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6336, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0206, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9795, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0641, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4487, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9086, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0393, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2243, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0733, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8033, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9316, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6637, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9451, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8863, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0640, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1686, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.6002, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0705, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2326, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3319, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1100, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8819, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1866, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3669, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0774, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4495, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5598, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3463, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9970, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5923, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5050, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5994, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4766, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8834, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.7455, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9942, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4122, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8788, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.8800, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9341, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2448, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4195, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5855, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5652, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3335, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2532, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.1765, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4103, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9555, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7413, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.8669, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6980, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6855, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3863, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6224, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9207, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9512, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0042, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2434, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.6984, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.1994, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2237, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.4077, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.6191, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.3265, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7435, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2177, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9102, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5665, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3242, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9283, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7106, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6717, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6251, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9987, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0174, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.8161, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0809, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.6058, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2989, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5755, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(2.9775, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.8920, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.3308, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0908, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0240, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0626, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7754, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.8439, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.3667, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2703, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0169, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4141, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.4079, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2168, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.3873, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7959, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6855, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7644, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.0323, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4841, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.4783, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7754, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9128, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.9836, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.0612, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.6608, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.9083, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2688, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0057, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2545, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0799, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7508, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7016, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.1121, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.3669, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.7839, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6642, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.5297, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.4905, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.8349, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.8750, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.4060, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.6689, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.8724, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.2010, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7057, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.0856, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.3161, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.1067, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.6471, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.2096, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.9453, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.5573, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.7582, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.9647, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.7191, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.0749, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2770, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.8898, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.5160, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.2624, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.5828, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.8689, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.8718, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.8187, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.6733, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.1376, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9701, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9605, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.5123, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.4337, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9400, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.3561, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.1497, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.6043, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.1216, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.4950, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.1399, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.1345, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9479, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.3809, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.2339, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0167, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.9413, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.8131, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.0378, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.3614, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.8332, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.8419, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.4225, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.3780, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.2611, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.0046, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.2325, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.4724, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.4632, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.8353, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.3666, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.1700, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.6820, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.1535, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.5850, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.5813, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.1939, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.3031, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.9441, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.5595, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.1409, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(3.8847, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.1417, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.0208, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.8954, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.1573, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.4713, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.3377, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.2033, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.5902, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7137, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.2863, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.0898, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.0930, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.0934, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.8862, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.3659, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.1075, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.1520, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.4506, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.5850, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.5885, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.7358, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.9192, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.1959, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.6277, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7234, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.1255, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.4773, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.4656, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.3721, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.1616, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.0474, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.3422, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(9.4088, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(6.8845, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.3545, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(12.5828, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.8493, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.5637, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.9788, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.4759, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(4.7971, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.2420, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.5935, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.0523, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.1671, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.0839, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.8551, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.6487, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.2988, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(5.9069, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.8634, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(7.7179, grad_fn=<NllLossBackward0>)\n",
      "loss=tensor(8.8223, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# store the learning rates and losses\n",
    "l_rates = []\n",
    "losses = []\n",
    "\n",
    "for step in range(1000):\n",
    "    # forward pass\n",
    "    ix = torch.randint(0, X.shape[0], (32, ))\n",
    "    # make sure to only select the indices in the current mini batch\n",
    "    embs = C[X[ix]]\n",
    "    h = torch.tanh(embs.view(-1, block_size*dims) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    # only select the indices in the current mini batch\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    # backward pass\n",
    "    # set the gradients to 0 - IMP\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    \n",
    "    # calculate the gradients of the loss w.r.t params\n",
    "    loss.backward()\n",
    "    \n",
    "    l_rate = lrs[step]\n",
    "    # update the params\n",
    "    for p in params:\n",
    "        # l_rate = 0.1 # learning rate \n",
    "        p.data += -l_rate * p.grad\n",
    "\n",
    "    # track the learning rate and loss\n",
    "    l_rates.append(l_rate)\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets plot the learning rates and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13f97f110>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(l_rates, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "we can seee from the above plot that the learning rate stabilized somewhere around 0.1-0.2 and then became unstable again after that \n",
    "\n",
    "so 0.1 seems like a good value for the learning rate, which we have tried initially!\n",
    "\n",
    "So we can choose a learning rate by doing this exercise!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.4 ('genAI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdc7a2fc295da552623d6189d2d73e88b479f44ab32ab05a73d51a9a8b2f91f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
